{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33eecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d159d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2e3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d8a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x,t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 64)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 32)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.sa3 = SelfAttention(512, 16)\n",
    "\n",
    "        self.bot1 = DoubleConv(512, 512)\n",
    "        #self.bot2 = DoubleConv(256, 256)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(256,8)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(128, 16)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(32, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)#nn.Sequential(nn.Conv2d(32, c_out, kernel_size=1),\n",
    "                                 #nn.Sigmoid())\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        #print(x.size())\n",
    "                           #Bx C x H x W\n",
    "        x1 = self.inc(x)   # B x 64 x 128 x 128\n",
    "        \n",
    "        x2 = self.down1(x1,t) #  B x 128 x  64 x 64\n",
    "        \n",
    "        #x2 = self.sa1(x2)     #  B x 128 x  32 x 32\n",
    "        #print(x2.size())\n",
    "        x3 = self.down2(x2,t) #  B x 256 x  16 x 16\n",
    "        \n",
    "        #x3 = self.sa2(x3)      #  B x 256 x  16 x 16\n",
    "        x4 = self.down3(x3,t) #  B x 512 x 8 x 8\n",
    "        #x4 = self.sa3(x4)    #  B x 512 x  8 x 8\n",
    "        \n",
    "        x4 = self.bot1(x4)   #  B x 512 x  8 x 8\n",
    "        #x4 = self.bot2(x4)   #  B x 256 x  4 x 4\n",
    "        x4 = self.bot3(x4)   #  B x 128 x  4 x 4\n",
    "        \n",
    "        x = self.up1(x4, x3,t) #  B x 256 x  8x 8\n",
    "        \n",
    "        #x = self.sa4(x)        #  B x 256 x  8x 8\n",
    "        x = self.up2(x, x2,t)  #  B x 128 x  16 x 16\n",
    "        #x = self.sa5(x)   #  B x 128 x  16 x 16\n",
    "        x = self.up3(x, x1,t) #  B x 64 x  32 x 32\n",
    "        #x = self.sa6(x)   #  B x 64 x  64 x 64\n",
    "        output = self.outc(x)  #  B x 3 x  128 x 128\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fba97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKGM(nn.Module):\n",
    "    def __init__(self,device,T=20):\n",
    "        super(DKGM, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.unet=UNet()\n",
    "        \n",
    "\n",
    "        self.T=T\n",
    "\n",
    "        self.device=device\n",
    "\n",
    "       \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n",
    "        #initial encoding\n",
    "        t0=torch.zeros(x.size(dim=0),device=self.device)\n",
    "\n",
    "        #initial decoding\n",
    "        reconstruction=self.unet(x,t0)\n",
    "\n",
    "        total_recons=reconstruction\n",
    "        bias=-reconstruction+x\n",
    "        recons_bias=torch.zeros_like(x, device=self.device)\n",
    "        a_i=0\n",
    "        #sequence of bias encoding +decoding \n",
    "        for i in range(self.T):\n",
    "\n",
    "            \n",
    "\n",
    "            bias=bias-recons_bias*a_i\n",
    "\n",
    "            recons_bias=self.unet(bias,t0+i+1)\n",
    "            a_i=1/(i+1.0)\n",
    "            total_recons+=recons_bias*a_i\n",
    "\n",
    "\n",
    "        return total_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c74687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def final_lossDKGM(mse_loss):\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "def model_trainDKGM(model,dataloader,dataset,device,optimizer,criterion,a=1):\n",
    "    model.train()\n",
    "    running_loss=0.0\n",
    "    counter=0\n",
    "    for i, data in tqdm(enumerate(dataloader),total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter+=1\n",
    "        data=data[0]\n",
    "        data=data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        posterior_X=model(data+a*torch.randn_like(data))\n",
    "\n",
    "        bce_loss= criterion(posterior_X,data)\n",
    "        \n",
    "        loss=final_lossEVAE(bce_loss)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    train_loss=running_loss/counter\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def model_validateDKGM(model,dataloader,dataset,device,optimizer,criterion,a=1):\n",
    "    model.eval()\n",
    "    running_loss=0.0\n",
    "    counter=0\n",
    "    with torch.no_grad():\n",
    "        for i,data in tqdm(enumerate(dataloader),total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter+=1\n",
    "            data=data[0]\n",
    "            data=data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            posterior_X=model(data+a*torch.randn_like(data))\n",
    "\n",
    "            bce_loss= criterion(posterior_X,data)\n",
    "            loss=bce_loss\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            if i==int(len(dataset)/dataloader.batch_size)-1:\n",
    "                recon_images=posterior_X\n",
    "                #noisez_image=data+noise\n",
    "        valid_loss=running_loss/counter\n",
    "        return valid_loss,recon_images\n",
    "        #return valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b980d",
   "metadata": {},
   "source": [
    "# training DKGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72a8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "lr=0.0003\n",
    "epochs=10\n",
    "batch_size=20\n",
    "image_size=128\n",
    "DKGMmodel =DKGM(T=0,device=device,).to(device) #UNet().to(device)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "                                      transforms.CenterCrop((160, 160)),\n",
    "                                  transforms.Resize((image_size,image_size)),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                       std=[0.5, 0.5, 0.5])\n",
    "                              ])\n",
    "\n",
    "from datasets import load_dataset\n",
    "def annot(examples):\n",
    "    examples['image'] = [transform(img) for img in examples['image']]\n",
    "    return examples\n",
    "ds = load_dataset(\"tglcourse/lsun_church_train\")\n",
    "ds = ds.with_format(\"torch\")\n",
    "#training set\n",
    "\n",
    "\n",
    "train_set = concatenate_datasets([ds['train'], ds['test']])\n",
    "train_set.set_transform(annot)\n",
    "train_loader=torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "optimizerDKGM=optim.Adam(DKGMmodel.parameters(),lr=lr)\n",
    "\n",
    "criterion=torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2994894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [36:00,  2.92it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:21705.2320\n",
      "Epoch2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:50,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:18791.0412\n",
      "Epoch3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:52,  2.93it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:18178.3566\n",
      "Epoch4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:54,  2.93it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17818.0336\n",
      "Epoch5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:48,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17577.5727\n",
      "Epoch6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:48,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17416.6533\n",
      "Epoch7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:46,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17295.4895\n",
      "Epoch8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:46,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17193.9203\n",
      "Epoch9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:46,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17115.5889\n",
      "Epoch10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:44,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:17048.1982\n",
      "Epoch11 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:45,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:16990.9659\n",
      "Epoch12 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:45,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:16937.1441\n",
      "Epoch13 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:45,  2.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:16892.0194\n",
      "Epoch14 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [35:42,  2.95it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:16848.9042\n",
      "Epoch15 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3811/6311 [21:34<14:09,  2.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     train_epoch_lossVAE=model_trainVAE(VAEmodel,train_loader,train_set,device,optimizerVAE,criterion)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     valid_epoch_lossVAE=model_validateVAE(VAEmodel,test_loader,test_set,device,optimizerVAE,criterion)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     train_lossVAE.append(train_epoch_lossVAE)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#     valid_lossVAE.append(valid_epoch_lossVAE)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     train_epoch_lossEVAE,recon_images\u001b[38;5;241m=\u001b[39mmodel_trainEVAE(DKGMmodel,train_loader,train_set,device,optimizerDKGM,criterion,a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#valid_epoch_lossEVAE,recon_images=model_validateEVAE(DKGMmodel,test_loader,test_set,device,optimizerDKGM,criterion,a=0.5,m=1)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     train_lossEVAE\u001b[38;5;241m.\u001b[39mappend(train_epoch_lossEVAE)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mmodel_trainEVAE\u001b[1;34m(model, dataloader, dataset, device, optimizer, criterion, a, m)\u001b[0m\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 33\u001b[0m running_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;66;03m#+errD_fake.item()+errD_real.item()\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m/\u001b[39mdataloader\u001b[38;5;241m.\u001b[39mbatch_size)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     35\u001b[0m     recon_images\u001b[38;5;241m=\u001b[39mposterior_X        \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_imagesDKGM=[]\n",
    "train_lossDKGM=[]\n",
    "valid_lossDKGM=[]\n",
    "\n",
    "grid_imagesDKGM=[]\n",
    "train_lossDKGM=[]\n",
    "valid_lossDKGM=[]\n",
    "for epoch in range(20):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "\n",
    "    train_epoch_lossDKGM,recon_images=model_trainDKGM(DKGMmodel,train_loader,train_set,device,optimizerDKGM,criterion,a=1,m=1)\n",
    "\n",
    "    train_lossDKGM.append(train_epoch_lossDKGM)\n",
    "\n",
    "    #save_reconstructed_images(recon_images,epoch+1)\n",
    "    print(f\"train loss:{train_epoch_lossEVAE:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2ad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f\"DKGM_LSUN{epoch}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbd785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_blur = transforms.Compose([\n",
    "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.8,1.2))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "DKGMmodel2 =DKGM(T=3,device=device,m=10).to(device) #UNet().to(device)\n",
    "\n",
    "\n",
    "optimizerDKGM2=optim.Adam(DKGMmodel2.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss=1000000000\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    DKGMmodel2.train()\n",
    "    running_loss=0.0\n",
    "    counter=0\n",
    "    for i, data in tqdm(enumerate(train_loader),total=int(len(train_set)/train_loader.batch_size)):\n",
    "        counter+=1\n",
    "        data=data['image']\n",
    "        data=data.to(device)\n",
    "        optimizerDKGM2.zero_grad()\n",
    "\n",
    "\n",
    "        posterior_Xt_1=DKGMmodel2(transform_blur(data))\n",
    "        #print(reconstruction)\n",
    "\n",
    "        mse_loss= criterion(posterior_Xt_1,data)\n",
    "\n",
    "        loss=final_lossDKGM(mse_loss)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizerDKGM2.step()\n",
    "\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "\n",
    "    train_loss=running_loss/counter\n",
    "\n",
    "    print(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c0a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7399/15778 [31:57<36:10,  3.86it/s]  "
     ]
    }
   ],
   "source": [
    "#VAEmodel.eval()\n",
    "DKGMmodel.eval()\n",
    "DKGMmodel2.eval()\n",
    "running_loss=0.0\n",
    "counter=0\n",
    "#test set\n",
    "tota_sharpDKGM=0.0\n",
    "tota_sharpDKGM=0.0\n",
    "from scipy import signal\n",
    "#laplace\n",
    "kernel=np.array([[0 ,1, 0],[1, -4,1],[0, 1 ,0]])\n",
    "\n",
    "\n",
    "from torcheval.metrics import FrechetInceptionDistance\n",
    "\n",
    "fidDKGM = FrechetInceptionDistance(device=device)            \n",
    "transform_grayscale=transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore(normalize=True)   \n",
    "with torch.no_grad():\n",
    "    for i,data in tqdm(enumerate(train_loader),total=int(len(train_set)/train_loader.batch_size)):\n",
    "        counter+=1\n",
    "        data=data[0]\n",
    "        data=data.to(device)\n",
    "        #optimizerVAE.zero_grad()\n",
    "        optimizerDKGM.zero_grad()\n",
    "\n",
    "        noise=torch.randn_like(data)\n",
    "\n",
    "        state1=DKGMmodel(data+1*noise)\n",
    "        reconstruction_DKGM=DKGMmodel2(state1)\n",
    "\n",
    "        image_grid_DKGM=transform_grayscale(torch.clamp((reconstruction_DKGM+1.0)/2,0,1)).detach().cpu()\n",
    "        for j in range(data.size(dim=0)):\n",
    "\n",
    "\n",
    "            sharpnessDKGM = np.var(np.abs(signal.convolve2d(image_grid_DKGM[j][0], kernel, mode=\"same\")))\n",
    "            tota_sharpDKGM+=sharpnessDKGM\n",
    "\n",
    "        inception.update(torch.clamp((reconstruction_DKGM.cpu()+1.0)/2,0.0,1.0))\n",
    "        \n",
    "        \n",
    "        fidEVAE.update(torch.clamp((data+1.0)/2,0,1), is_real=True)\n",
    "        fidEVAE.update(torch.clamp((reconstruction_DKGM+1.0)/2,0,1), is_real=False)\n",
    "        \n",
    "\n",
    "lossDKGM=fidDKGM.compute()\n",
    "Is=inception.compute()\n",
    "# print(f\"FIDVAE: {float(lossVAE)}\")\n",
    "# print(f\"shaprnessVAE:{tota_sharpVAE/len(test_set):.4f}\")\n",
    "print(f\"FIDDKGM: {float(lossDKGM)}\")\n",
    "print(f\"shaprnessDKGM:{tota_sharpDKGM/len(train_set):.4f}\")\n",
    "print(f\"IS (mean): {float(Is[0])}\")\n",
    "print(f\"IS (std): {float(Is[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "393f9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 44870237\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # trainable parameters: sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb974817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
