{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33eecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6259454c-ede5-40ea-bb9f-c06ed4b7782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\63422\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\63422\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 3.7/8.0 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 29.3 MB/s eta 0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.9.2\n",
      "    Uninstalling matplotlib-3.9.2:\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\users\\\\63422\\\\anaconda3\\\\lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp312-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d2e3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "def image_to_vid(images):\n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    imageio.mimsave('../outputs/generated_images.gif', imgs)\n",
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    save_image(recon_images.cpu(), f\"{epoch}.jpg\")\n",
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.jpg')\n",
    "    plt.show()\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d8a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encodercnn(nn.Module):\n",
    "    def __init__(self,input_dim=2,out_dim=2,latent_dim=1):\n",
    "        super(Encodercnn,self).__init__()\n",
    "\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(input_dim,2*input_dim),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(2*input_dim,4*input_dim),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim*4, 1*latent_dim),            \n",
    "\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1c69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decodercnn(nn.Module):\n",
    "    def __init__(self, input_dim=2,out_dim=2,latent_dim=1):\n",
    "        super(Decodercnn,self).__init__()\n",
    "        \n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim*2),  \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim*2,4*input_dim),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(input_dim*4, out_dim),            \n",
    "\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.dec(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae45e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKGM(nn.Module):\n",
    "    def __init__(self, N,device,input_dim=2,out_dim=2,latent_dim=1,m=100,T=20,a=0):\n",
    "        super(DKGM, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        #initial CNN\n",
    "        self.encoder_initial=Encodercnn(input_dim=input_dim,out_dim=out_dim,latent_dim=latent_dim)\n",
    "        self.decoder_initial=Decodercnn(input_dim=input_dim,out_dim=out_dim,latent_dim=latent_dim)\n",
    "        self.bm=np.power(m,-2/9)\n",
    "        self.T=T\n",
    "        self.N=N\n",
    "        self.a=a\n",
    "        self.device=device\n",
    "        \n",
    "\n",
    "        #list of bias encoders\n",
    "        self.bias_encoders=nn.ModuleList()\n",
    "        for i in range(self.T):\n",
    "            self.bias_encoders.append(Encodercnn(input_dim=input_dim,out_dim=out_dim,latent_dim=latent_dim))\n",
    "        #list of bias decoders\n",
    "        self.bias_decoders=nn.ModuleList()\n",
    "        for i in range(self.T):\n",
    "            self.bias_decoders.append(Decodercnn(input_dim=input_dim,out_dim=out_dim,latent_dim=latent_dim))\n",
    "  \n",
    "    def reparametrize(self,mu,log_r,log_beta):\n",
    "        \n",
    "        coef= self.bm #b(m)\n",
    "        n=log_r.size(dim=0)\n",
    "        d=log_r.size(dim=1)\n",
    "        beta=torch.exp(0.5*log_beta)\n",
    "        uniform_k=torch.rand(n*d,3, device=self.device)\n",
    "        U_k=2*uniform_k-1\n",
    "        \n",
    "        uniform_p=torch.rand(n,d, device=self.device)\n",
    "        U_p=2*uniform_p-1  #uniform prior\n",
    "        \n",
    "        median=torch.median(U_k,1)[0].reshape(n,d) #sample from E kernel\n",
    "        sample=coef*(torch.exp(0.5*log_r)*median+mu)+beta*U_p\n",
    "        return sample\n",
    "       \n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.a*x+2\n",
    "        #initial encoding\n",
    "        enc=self.encoder_initial(x)\n",
    "\n",
    "\n",
    "        #initial decoding\n",
    "        reconstruction=self.decoder_initial(enc)\n",
    "        \n",
    "        #initialization of sequence EVAE\n",
    "        total_recons=reconstruction\n",
    "        bias=x-reconstruction\n",
    "        recons_bias=torch.zeros_like(x, device=self.device)\n",
    "        a_i=0\n",
    "        #sequence of bias encoding +decoding \n",
    "        for i in range(self.T):\n",
    "            bias_encoder=self.bias_encoders[i]\n",
    "            bias_decoder=self.bias_decoders[i]\n",
    "            \n",
    "            bias=bias-recons_bias*a_i  \n",
    "            z_b= bias_encoder(bias)\n",
    "\n",
    "            recons_bias=bias_decoder(z_b)   \n",
    "            a_i=1/(i+1.0)\n",
    "            total_recons+=recons_bias*a_i\n",
    "\n",
    "\n",
    "        return total_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc27c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss):\n",
    "    #para1[0]: log_r  para1[1]:log_beta\n",
    "\n",
    "    return mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6203b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model,dataloader,dataset,device,optimizer,criterion):\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    running_loss=0.0\n",
    "    counter=0\n",
    "    for i, data in tqdm(enumerate(dataloader),total=int(len(dataset)/dataloader.batch_size)):\n",
    "        counter+=1\n",
    "        #data=data[0]\n",
    "        data=data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction=model(data)\n",
    "\n",
    "        mse_loss= criterion(reconstruction,data)\n",
    "\n",
    "        loss=final_loss(mse_loss)\n",
    "        #print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        running_loss+=loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss=running_loss/counter\n",
    "    #print(log_var)\n",
    "    return train_loss\n",
    "  \n",
    "\n",
    "def model_validate(model,dataloader,dataset,device,criterion,optimizer):\n",
    "\n",
    "    model.eval()\n",
    "    running_loss=0.0\n",
    "    counter=0\n",
    "    with torch.no_grad():\n",
    "        for i,data in tqdm(enumerate(dataloader),total=int(len(dataset)/dataloader.batch_size)):\n",
    "            counter+=1\n",
    "            #data=data[0]\n",
    "            data=data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction=model(data)\n",
    "            mse_loss= criterion(reconstruction,data)\n",
    "\n",
    "            loss=mse_loss#+bce_loss_bias#final_loss(bce_loss,mu,log_var)\n",
    "            running_loss+=loss.item()\n",
    "            if i==int(len(dataset)/dataloader.batch_size)-1:\n",
    "                recon_images=reconstruction\n",
    "        valid_loss=running_loss/counter\n",
    "        return valid_loss,recon_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b980d",
   "metadata": {},
   "source": [
    "# training DKGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "#Parameters to set\n",
    "lambd=1\n",
    "n=20000\n",
    "alpha=4*np.pi/3\n",
    "u=np.random.exponential(lambd, n) \n",
    "\n",
    "phi=np.random.normal(np.pi/2, np.sqrt(np.pi/4),n)\n",
    "\n",
    "X=np.zeros((n, 2))\n",
    "for i in range(n):\n",
    "    \n",
    "    x = alpha*np.sqrt(u[i])/3 *np.cos(alpha*np.sqrt(u[i]))\n",
    "    y = alpha*np.sqrt(u[i])/3 *np.sin(alpha*np.sqrt(u[i]))\n",
    "    X[i,0]=x\n",
    "    X[i,1]=y\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c =\"blue\")\n",
    "\n",
    "# # To show the plot\n",
    "plt.show()\n",
    "\n",
    "X=Variable(torch.from_numpy(X).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ca94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=X[:,0],y=X[:,1],ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f5edea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72a8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "lr=0.0005\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model_kernel=DKGM(latent_dim=1,N=batch_size,T=30,device=device,a=0.1).to(device)\n",
    "model_boost=DKGM(latent_dim=1,N=batch_size,T=30,device=device,a=0).to(device)\n",
    "train_loader=torch.utils.data.DataLoader(X,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "\n",
    "optimizer_kernel=optim.Adam(model_kernel.parameters(),lr=lr)\n",
    "optimizer_boost=optim.Adam(model_boost.parameters(),lr=lr)\n",
    "#scheduler=optim.lr_scheduler.StepLR(optimizer,epochs//4,gamma=0.3)\n",
    "criterion=torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2994894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:184.5380\n",
      "Epoch2 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:157.3244\n",
      "Epoch3 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:100.6604\n",
      "Epoch4 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:60.2186\n",
      "Epoch5 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:21.1327\n",
      "Epoch6 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:9.5507\n",
      "Epoch7 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:5.1789\n",
      "Epoch8 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:3.0450\n",
      "Epoch9 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 34.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.9698\n",
      "Epoch10 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.3834\n",
      "Epoch11 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0617\n",
      "Epoch12 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8457\n",
      "Epoch13 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6952\n",
      "Epoch14 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.5834\n",
      "Epoch15 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.4992\n",
      "Epoch16 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.4316\n",
      "Epoch17 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.3702\n",
      "Epoch18 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.3385\n",
      "Epoch19 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2836\n",
      "Epoch20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2584\n",
      "Epoch21 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2232\n",
      "Epoch22 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1966\n",
      "Epoch23 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1819\n",
      "Epoch24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1586\n",
      "Epoch25 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1489\n",
      "Epoch26 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1326\n",
      "Epoch27 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 30.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1246\n",
      "Epoch28 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1082\n",
      "Epoch29 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0950\n",
      "Epoch30 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 30.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0872\n",
      "Epoch31 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 29.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0837\n",
      "Epoch32 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [00:00<00:06, 28.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     train_epoch_loss\u001b[38;5;241m=\u001b[39mmodel_train(model_kernel,train_loader,X,device,optimizer_kernel,criterion)\n\u001b[0;32m     12\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(model, dataloader, dataset, device, optimizer, criterion)\u001b[0m\n\u001b[0;32m     10\u001b[0m data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m reconstruction\u001b[38;5;241m=\u001b[39mmodel(data)\n\u001b[0;32m     15\u001b[0m mse_loss\u001b[38;5;241m=\u001b[39m criterion(reconstruction,data)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m=\u001b[39mfinal_loss(mse_loss)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 64\u001b[0m, in \u001b[0;36mDKGM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m bias_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_decoders[i]\n\u001b[0;32m     63\u001b[0m bias\u001b[38;5;241m=\u001b[39mbias\u001b[38;5;241m-\u001b[39mrecons_bias\u001b[38;5;241m*\u001b[39ma_i  \n\u001b[1;32m---> 64\u001b[0m z_b\u001b[38;5;241m=\u001b[39m bias_encoder(bias)\n\u001b[0;32m     66\u001b[0m recons_bias\u001b[38;5;241m=\u001b[39mbias_decoder(z_b)   \n\u001b[0;32m     67\u001b[0m a_i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mEncodercnn.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:392\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_images=[]\n",
    "train_loss=[]\n",
    "valid_loss=[]\n",
    "mus=[]\n",
    "varss=[]\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_kernel,train_loader,X,device,optimizer_kernel,criterion)\n",
    "\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f454f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_X=model_kernel(X.to(device))\n",
    "biased_image=0.1*X.to(device)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57a73055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='None', ylabel='None'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X.cpu().detach().numpy()[:,0],y=reconstruct_X.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d129acdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='None', ylabel='None'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax=plt.gca()\n",
    "sns.scatterplot(x=biased_image.cpu().detach().numpy()[:,0],y=biased_image.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model_0=DKGM(latent_dim=1,N=batch_size,T=0,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer0=optim.Adam(model_0.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_0,train_loader,X,device,optimizer0,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    #valid_loss.append(valid_epoch_loss)\n",
    "  #  mus.append(mu)\n",
    "   # varss.append(logvar.exp())\n",
    "    #save images recon\n",
    "    #save_reconstructed_images(recon_images,epoch+1)\n",
    "\n",
    "\n",
    "    #grid_images.append(image_grid)\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "    #print(f\"valid loss:{valid_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb39715",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_X0=model_0(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X0.cpu().detach().numpy()[:,0],y=reconstruct_X0.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46999e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model_1=DKGM(latent_dim=1,N=batch_size,T=1,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer1=optim.Adam(model_1.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_1,train_loader,X,device,optimizer1,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "    #print(f\"valid loss:{valid_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_X1=model_1(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X1.cpu().detach().numpy()[:,0],y=reconstruct_X1.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0001\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_2=DKGM(latent_dim=1,N=batch_size,T=2,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer2=optim.Adam(model_2.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_2,train_loader,X,device,optimizer2,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "reconstruct_X2=model_2(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X2.cpu().detach().numpy()[:,0],y=reconstruct_X2.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb28f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0001\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_3=DKGM(latent_dim=1,N=batch_size,T=3,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer3=optim.Adam(model_3.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_3,train_loader,X,device,optimizer3,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X3=model_3(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X3.cpu().detach().numpy()[:,0],y=reconstruct_X3.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31166e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0001\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_4=DKGM(latent_dim=1,N=batch_size,T=4,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer4=optim.Adam(model_4.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_4,train_loader,X,device,optimizer4,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X4=model_4(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X4.cpu().detach().numpy()[:,0],y=reconstruct_X4.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76043926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model_5=DKGM(latent_dim=1,N=batch_size,T=5,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer5=optim.Adam(model_5.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_5,train_loader,X,device,optimizer5,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X5=model_5(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X5.cpu().detach().numpy()[:,0],y=reconstruct_X5.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee394358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model_10=DKGM(latent_dim=1,N=batch_size,T=10,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer10=optim.Adam(model_10.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_10,train_loader,X,device,optimizer10,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X10=model_10(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X10.cpu().detach().numpy()[:,0],y=reconstruct_X10.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_15=DKGM(latent_dim=1,N=batch_size,T=15,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer15=optim.Adam(model_15.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_15,train_loader,X,device,optimizer15,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X15=model_15(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X15.cpu().detach().numpy()[:,0],y=reconstruct_X15.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f68a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=0.0005\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_20=DKGM(latent_dim=1,N=batch_size,T=20,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer20=optim.Adam(model_20.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_20,train_loader,X,device,optimizer20,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X20=model_20(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X20.cpu().detach().numpy()[:,0],y=reconstruct_X20.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa88b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DKGM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      3\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model_25\u001b[38;5;241m=\u001b[39mDKGM(latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,N\u001b[38;5;241m=\u001b[39mbatch_size,T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice,a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer25\u001b[38;5;241m=\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_25\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DKGM' is not defined"
     ]
    }
   ],
   "source": [
    "lr=0.0005\n",
    "epochs=50\n",
    "batch_size=100\n",
    "\n",
    "model_25=DKGM(latent_dim=1,N=batch_size,T=25,device=device,a=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer25=optim.Adam(model_25.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch{epoch+1} of {epochs}\")\n",
    "    train_epoch_loss=model_train(model_25,train_loader,X,device,optimizer25,criterion)\n",
    "    #valid_epoch_loss,recon_images=model_validate(model,test_loader,test_set,device,criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"train loss:{train_epoch_loss:.4f}\")\n",
    "\n",
    "    \n",
    "reconstruct_X25=model_25(X.to(device))\n",
    "ax=plt.gca()\n",
    "sns.scatterplot(x=reconstruct_X25.cpu().detach().numpy()[:,0],y=reconstruct_X25.cpu().detach().numpy()[:,1],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc1523-3321-41bb-8951-f46dceb802b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d48efd-ddbc-43d8-8872-c4888a269683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
